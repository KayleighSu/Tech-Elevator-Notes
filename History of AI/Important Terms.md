## History of AI:

*Neural Network:*
A computational model inspired by the structure of the human brian, consisting of interconnected nodes (neurons). It processes and learns from data by adjusting the connections between these nodes to recognize patterns, make predictions, or perform various tasks.

*Machine Learning:*
A general term for having computers solve problems by discovering algorithms through training without needing to be explicitly told what to do.

*Big data:*
Enormous datasets that often include vast amounts of informtion from diverse sources with varying and often complex structures.

*Natural Language Processing (NLP):*
A subfield of linguistics and computer science focused on processing natural language (speech and text) to understand content correctly in context.

*Language Models and Large Language Models (LLMs):*
Learn patterns and relationships from vast amounts of training data to build a natural language model that allows them to comprehend, predict, and generate coherent human language.

*Deep Learning:*
A subfield of machine learning based on multi-layered neural networks.

*Generative AI:*
A form of AI capable of generating content like images, text, or even program code from prompts. Examples include OpenAI's ChatGPT and DALL-E, Google Bard, Meta's LLaMA, Midjourney, and Stable Diffusion.

## Generative AI:

*Generative AI:*
A cutting-edge branch of AI that focuses on creating new content.

*Hallucinations:*
The incorrect outputs created by Generative AI as it lacks genuine understanding.
